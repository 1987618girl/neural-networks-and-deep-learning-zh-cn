# 第二章 反向传播算法如何工作的？

在上一章，我们看到了神经网络如何使用梯度下降算法来学习他们自身的权重和偏差。但是，这里还留下了一个问题：我们并没有讨论如何计算代价函数的梯度。这是很大的缺失！在本章，我们会解释计算这些梯度的快速算法，也就是**反向传播**。

反向传播算法最初在 1970 年代被发现，但是这个算法的重要性直到  [David Rumelhart](http://en.wikipedia.org/wiki/David_Rumelhart), [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/), 和 [Ronald Williams](http://en.wikipedia.org/wiki/Ronald_J._Williams) 的 [1986年的论文](http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf)中才被真正认可。这篇论文描述了对一些神经网络反向传播要比传统的方法更快，这使得使用神经网络来解决之前无法完成的问题变得可行。现在，反向传播算法已经是神经网络学习的重要组成部分了。

本章在全书的范围内要比其他章节包含更多的数学内容。如果你不是对数学特别感兴趣，那么可以跳过本章，将反向传播当成一个黑盒，忽略其中的细节。那么为何要研究这些细节呢？

答案当然是理解。反向传播的核心是对代价函数 $$C$$ 关于 $$w$$ （或者 $$b$$）的偏导数 $$\partial C/\partial w$$ 的计算表示。该表示告诉我们在权重和偏差发生改变时，代价函数变化的快慢。尽管表达式会有点复杂，不过里面也包含一种美感，就是每个元素其实是拥有一种自然的直觉上的解释。所以反向传播不仅仅是一种学习的快速算法。实际上它还告诉我们一些细节的关于权重和偏差的改变影响整个网络行为方面的洞察。因此，这也是学习反向传播细节的重要价值所在。

如上面所说，如果你想要粗览本章，或者直接跳到下一章，都是可以的。剩下的内容即使你是把反向传播看做黑盒也是可以掌握的。当然，后面章节中也会有部分内容涉及本章的结论，所以会常常给出本章的参考。不过对这些知识点，就算你对推导的细节不太清楚你还是应该要理解主要的结论的。

# 热身：神经网络中使用矩阵快速计算输出的观点
---
在讨论反向传播前，我们先熟悉一下基于矩阵的算法来计算网络的输出。事实上，我们在上一章的最后已经能够看到这个算法了，但是我在那里很快地略过了，所以现在让我们仔细讨论一下。特别地，这样能够用相似的场景帮助我们熟悉在反向传播中使用的矩阵表示。

我们首先给出网络中权重的清晰定义。我们使用 $$w_{jk}^l$$ 表示从 $$(l-1)^{th}$$ 层的 $$k^{th}$$ 个神经元到 $$(l)^{th}$$ 层的 $$l^{th}$$ 个神经元的链接上的权重。例如，下图给出了第二隐藏层的第四个神经元到第三隐藏层的第二个神经元的链接上的权重：

![](http://upload-images.jianshu.io/upload_images/42741-ff5e2fabf59a5b99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这样的表示粗看比较奇怪，需要花一点时间消化。但是，后面你会发现这样的表示会比较方便也很自然。奇怪的一点其实是下标 $$j$$ 和 $k$$ 的顺序。你可能觉得反过来更加合理。但我接下来会告诉你为什么要这样做。

我们对网络偏差和激活值也会使用类似的表示。显式地，我们使用 $$b_J^l$$ 表示在 $$l^{th}$$ 层 $$j^{th} $$ 个神经元的偏差，使用 $$a_j^l$$ 表示 $$l^{th}$$ 层 $$j^{th} $$ 个神经元的激活值。下面的图清楚地解释了这样表示的含义：

![](http://upload-images.jianshu.io/upload_images/42741-2ecac4fcb631edcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

有了这些表示， $$l^{th}$$ 层的 $$j^{th}$$ 个神经元的激活值 $$a_j^l$$ 就和 $$l^{th}$$ 层关联起来了（对比公式(4) 和上一章的讨论）

![](http://upload-images.jianshu.io/upload_images/42741-42c159f8a4e9fbe8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中求和是在 $$(l-1)^{th}$$ 层的所有神经元上进行的。为了用矩阵的形式重写这个表达式，我们对每一层 $$l$$ 都定义一个权重矩阵 $$w^l$$，在 $$j^{th}$$ 行第$$k^{th}$$ 列的元素是 $$w_{jk}^l$$。类似的，对每一层 $$l$$，定义一个偏差向量，$$b^l$$。你已经猜到这些如何工作了——偏差向量的每个元素其实就是前面给出的 $$b_j^l$$，每个元素对应于 $$l^{th}$$ 层的每个神经元。最后，我们定义激活向量 $$a^l$$，其元素是那些激活值 $$a_j^l$$。

最后我们需要引入向量化函数（如 $$\sigma$$）来按照矩阵形式重写公式(23) 。在上一章，我们其实已经碰到向量化了，其含义就是作用函数（如 $$\sigma$$）到向量 $$v$$ 中的每个元素。我们使用 $$\sigma(v)$$ 表示这种按元素进行的函数作用。所以，$$\sigma(v)$$ 的每个元素其实满足 $$\sigma(v)_j = \sigma(v_j)$$。给个例子，如果我们的作用函数是 $$f(x) = x^2$$，那么向量化的 $$f$$ 的函数作用就起到下面的效果：

![](http://upload-images.jianshu.io/upload_images/42741-f2f88546abd55d42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

也就是说，向量化的 $$f$$ 仅仅是对向量的每个元素进行了平方运算。

了解了这些表示，方程(23)就可以写成下面的这种美妙而简洁的向量形式了：

![](http://upload-images.jianshu.io/upload_images/42741-a70cba165118ba8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个表达式给出了一种更加全局的思考每层的激活值和前一层的关联方式：我们仅仅用权重矩阵作用在激活值上，然后加上一个偏差向量，最后作用 $$\sigma$$ 函数。
> 其实，这就是让我们使用之前的矩阵下标 $$w_{jk}^l$$ 表示的初因。如果我们使用 $$j$$ 来索引输入神经元，$$k$$ 索引输出神经元，那么在方程(25)中我们需要将这里的矩阵换做其转置。这只是一个小小的困惑的改变，这会使得我们无法自然地讲出（思考）“应用权重矩阵到激活值上”这样的简单的表达。

这种全局的观点相比神经元层面的观点常常更加简明（没有更多的索引下标了！）**其实可以看做是在保留清晰认识的前提下逃离下标困境的方法**。在实践中，表达式同样很有用，因为大多数矩阵库提供了实现矩阵乘法、向量加法和向量化的快速方法。实际上，上一章的[代码](http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits)其实已经隐式使用了使用这种表达式来计算网络行为。

在使用方程(25)计算 $$a^l$$ 时，我们计算了中间量 $$z^l \equiv w^la^{l-1} + b^l$$ 。这个量其实是非常有用的：我们称 $$z^l$$ 为 $$l$$ 层的带权输入。在本章后面，我们会大量用到这个量。方程(25)有时候会写作 $$a^l = \sigma(z^l)$$。同样要指出的是 $$z_l$$ 的每个元素是 $$z_j^l = \sum_k w_{jk}^l a_k^{l-1} + b_j^l$$，其实 $$z_j^l$$ 就是第 $$l$$ 层第 $$j$$ 个神经元的激活函数带权输入。

# 关于代价函数的两个假设
---
反向传播的目标是计算代价函数 $$C$$ 分别关于 $$w$$ 和 $$b$$ 的偏导数 $$\partial C/\partial w$$ 和 $$\partial C/\partial b$$。为了让反向传播可行，我们需要做出关于代价函数的两个主要假设。在给出这两个假设之前，我们先看看具体的一个代价函数。我们会使用上一章使用的二次代价函数。按照上一节给出的表示，二次代价函数有下列形式：

![](http://upload-images.jianshu.io/upload_images/42741-280efe3a58896b66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中 $$n$$ 是训练样本的总数；求和是在所有的训练样本 $$x$$ 上进行的；$$y = y(x)$$ 是对应的目标输出；$$L$$ 表示网络的层数；$$a^L = a^L(x)$$ 是当输入是 $$x$$ 时的网络输出的激活值向量。

好了，为了应用反向传播，我们需要对代价函数做出什么样的前提假设呢？第一个假设就是代价函数可以被写成一个 在每个训练样本 $$x$$ 上的代价函数 $$C_x$$ 的均值 $$C=\frac{1}{n}\sum_x  C_x$$。这是关于二次代价函数的例子，其中对每个独立的训练样本其代价是 $$C_x = \frac{1}{2} ||y-a^L||^2$$。这个假设对书中提到的其他任何一个代价函数也都是必须满足的。

需要这个假设的原因是反向传播实际上是对一个独立的训练样本计算了 $$\partial C_x/\partial w$$ 和 $$\partial C_x/\partial b$$。然后我们通过在所有训练样本上进行平均化获得 $$\partial C/\partial w$$ 和 $$\partial C/\partial b$$。实际上，有了这个假设，我们会认为训练样本 $$x$$ 已经被固定住了，丢掉了其下标，将代价函数 $$C_x$$ 看做 $$C$$。最终我们会把下标加上，现在为了简化表示其实没有这个必要。

第二个假设就是代价可以写成神经网络输出的函数：

![](http://upload-images.jianshu.io/upload_images/42741-9d3e685c6cfa4a3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

例如，二次代价函数满足这个要求，因为对于一个单独的训练样本 $$x$$ 其二次代价函数可以写作：

![](http://upload-images.jianshu.io/upload_images/42741-5e76d72976804bc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这是输出的激活值的函数。当然，这个代价函数同样还依赖于目标输出 $$y$$。记住，输入的训练样本 $$x$$ 是固定的，所以输出同样是一个固定的参数。所以说，并不是可以随意改变权重和偏差的，也就是说，这不是神经网络学习的对象。所以，将 C 看成仅有输出激活值 $$a^L$$ 的函数才是合理的，而 y 仅仅是帮助定义函数的参数而已。

# Hadamard 乘积，$$s\odot t$$
---
反向传播算法基于常规的线性代数运算——诸如向量加法，向量矩阵乘法等。但是有一个运算不大常见。特别地，假设 s 和 t 是两个同样维度的向量。那么我们使用     